{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hz2yJhDJhvSx"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import gym\n",
        "import pylab\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 200 정도로 수정해도 돌아가긴 하는 것 같습니다!\n",
        "EPISODES = 300"
      ],
      "metadata": {
        "id": "vSUOyNEPhyzz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [ code here ] 부분 빈칸을 채워주세요"
      ],
      "metadata": {
        "id": "XUl3mVgOi_l8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 카트폴 예제에서의 DQN 에이전트\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.render = False\n",
        "        self.load_model = False\n",
        "\n",
        "        # 상태와 행동의 크기 정의\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # DQN 하이퍼파라미터\n",
        "        self.discount_factor = 0.99\n",
        "        self.learning_rate = 0.001\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_decay = 0.999\n",
        "        self.epsilon_min = 0.01\n",
        "        self.batch_size = 64\n",
        "        self.train_start = 1000\n",
        "\n",
        "        # 문제 1\n",
        "        # 리플레이 메모리, 최대 크기 2000 (deque 사용)\n",
        "        self.memory = deque(maxlen=2000)\n",
        "\n",
        "        # 모델과 타깃 모델 생성\n",
        "        self.model = self.build_model()\n",
        "        self.target_model = self.build_model()\n",
        "\n",
        "        # 타깃 모델 초기화\n",
        "        self.update_target_model()\n",
        "\n",
        "        # if self.load_model:\n",
        "            # self.model.load_weights(\"./save_model/cartpole_dqn_trained.h5\")\n",
        "\n",
        "    # 상태가 입력, 큐함수가 출력인 인공신경망 생성\n",
        "    def build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(24, input_dim=self.state_size, activation='relu',\n",
        "                        kernel_initializer='he_uniform'))\n",
        "        model.add(Dense(24, activation='relu',\n",
        "                        kernel_initializer='he_uniform'))\n",
        "        model.add(Dense(self.action_size, activation='linear',\n",
        "                        kernel_initializer='he_uniform'))\n",
        "        model.summary()\n",
        "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    # 타깃 모델을 모델의 가중치로 업데이트\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    # 입실론 탐욕 정책으로 행동 선택\n",
        "    def get_action(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        else:\n",
        "            q_value = self.model.predict(state)\n",
        "            # 문제 2\n",
        "            # 최대 Q-가치를 고려하도록 행동 선택\n",
        "            return np.argmax(q_value[0])\n",
        "\n",
        "    # 샘플 <s, a, r, s'>을 리플레이 메모리에 저장\n",
        "    def append_sample(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    # 리플레이 메모리에서 무작위로 추출한 배치로 모델 학습\n",
        "    def train_model(self):\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "        # 메모리에서 배치 크기만큼 무작위로 샘플 추출\n",
        "        mini_batch = random.sample(self.memory, self.batch_size)\n",
        "\n",
        "        states = np.zeros((self.batch_size, self.state_size))\n",
        "        next_states = np.zeros((self.batch_size, self.state_size))\n",
        "        actions, rewards, dones = [], [], []\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            states[i] = mini_batch[i][0]\n",
        "            actions.append(mini_batch[i][1])\n",
        "            rewards.append(mini_batch[i][2])\n",
        "            next_states[i] = mini_batch[i][3]\n",
        "            dones.append(mini_batch[i][4])\n",
        "\n",
        "        # 현재 상태에 대한 모델의 큐함수\n",
        "        # 다음 상태에 대한 타깃 모델의 큐함수\n",
        "        target = self.model.predict(states)\n",
        "        target_val = self.target_model.predict(next_states)\n",
        "\n",
        "        # 문제 3\n",
        "        # 벨만 최적 방정식을 이용한 업데이트 타깃\n",
        "        ### 세 변수 a, s’, r 의 모든 조합에 대한 확률을 기대 가치에 곱해 더한 값\n",
        "        ### 빈칸은 기대 가치 = 보상 + 할인계수 * 다음 상태의 가치\n",
        "        for i in range(self.batch_size):\n",
        "            if dones[i]:\n",
        "                target[i][actions[i]] = rewards[i]\n",
        "            else:\n",
        "                target[i][actions[i]] = rewards[i] + self.discount_factor * (\n",
        "                    np.amax(target_val[i]))\n",
        "        self.model.fit(states, target, batch_size=self.batch_size,\n",
        "                       epochs=1, verbose=0)"
      ],
      "metadata": {
        "id": "0sc9q0Sih3UX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 문제는 끝났습니다.\n",
        "# 훈련이 오래 걸리는 점 유의해주세요!\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # CartPole-v1 환경, 최대 타임스텝 수가 500\n",
        "    env = gym.make('CartPole-v1')\n",
        "    state_size = env.observation_space.shape[0]\n",
        "    action_size = env.action_space.n\n",
        "\n",
        "    # DQN 에이전트 생성\n",
        "    agent = DQNAgent(state_size, action_size)\n",
        "\n",
        "    scores, episodes = [], []\n",
        "\n",
        "    for e in range(EPISODES):\n",
        "        done = False\n",
        "        score = 0\n",
        "        # env 초기화\n",
        "        state = env.reset()\n",
        "        state = np.reshape(state, [1, state_size])\n",
        "\n",
        "        while not done:\n",
        "            if agent.render:\n",
        "                env.render()\n",
        "\n",
        "            # 현재 상태로 행동을 선택\n",
        "            action = agent.get_action(state)\n",
        "            # 선택한 행동으로 환경에서 한 타임스텝 진행\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            next_state = np.reshape(next_state, [1, state_size])\n",
        "            # 에피소드가 중간에 끝나면 -100 보상\n",
        "            reward = reward if not done or score == 499 else -100\n",
        "\n",
        "            # 리플레이 메모리에 샘플 <s, a, r, s'> 저장\n",
        "            agent.append_sample(state, action, reward, next_state, done)\n",
        "            # 매 타임스텝마다 학습\n",
        "            if len(agent.memory) >= agent.train_start:\n",
        "                agent.train_model()\n",
        "\n",
        "            score += reward\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                # 각 에피소드마다 타깃 모델을 모델의 가중치로 업데이트\n",
        "                agent.update_target_model()\n",
        "\n",
        "                score = score if score == 500 else score + 100\n",
        "                # 에피소드마다 학습 결과 출력\n",
        "                scores.append(score)\n",
        "                episodes.append(e)\n",
        "                pylab.plot(episodes, scores, 'b')\n",
        "                # pylab.savefig(\"./save_graph/cartpole_dqn.png\")\n",
        "                print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
        "                      len(agent.memory), \"  epsilon:\", agent.epsilon)\n",
        "\n",
        "                # 이전 10개 에피소드의 점수 평균이 490보다 크면 학습 중단\n",
        "                if np.mean(scores[-min(10, len(scores)):]) > 490:\n",
        "                    # agent.model.save_weights(\"./save_model/cartpole_dqn.h5\")\n",
        "                    sys.exit()"
      ],
      "metadata": {
        "id": "QYRTDzwhh9HL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}